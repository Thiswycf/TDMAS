# running configuration

# general
# dataset: AIME24 # datasets: GSM8K, AIME24, GPQA
dataset: GSM8K # datasets: GSM8K, AIME24, GPQA
task_type: full_pipeline # full_pipeline, infer_only, train_only
start_epoch: 0 # start epoch
end_epoch: 5 # end epoch
train_ask_num: 8 # Number of repeated asks to train
test_ask_num: 5 # Number of repeated asks for stable test
max_depth: 5 # max depth of the problem
max_loop: 5 # max turn of a problem
max_debug_attempts: 2 # max debug attempts
ignore_first_evolution: true # ignore the Epoch 0 evolution

# model
model_name: Qwen3-8B # model name in config/local_llm.yaml
max_input_tokens: 8192 # tokens
max_output_tokens: 1024 # tokens
# max_input_tokens: 24576 # 24K tokens / `raise error` when `less than 20K` in `AIME24`
# max_output_tokens: 4096 # 4K qtokens
# declared_GPU_memory_usage: 38.8 # GB
declared_GPU_memory_usage: 24.0 # GB
temperature: 0.2 # temperatureï¼š 0.2/0.6/0.9

# train
similarity_threshold: 0.8 # similarity threshold for preference pairs
preference_pairs_limit: 128 # preference pairs per epoch

# test
# limit: 2 # limit samples to rapid test (problems number)
# limit: 128 # limit samples to rapid test (problems number)
limit: 10000 # No limit

# system
max_concurrent_request: 512 # max concurrent requests to the LLM
max_concurrent_execute_code: 128 # max concurrent requests to execute code

# optimize (PPO training)
optimize_config:
  policy_model: Qwen3-8B # Policy model name
  seed: 42 # Random seed
  max_prompt_len: 2048 # Max prompt length
  max_new_tokens: 1024 # Max new tokens to generate
  temperature: 0.7 # Sampling temperature
  top_p: 0.9 # Top-p sampling
  learning_rate: 1e-6 # Learning rate
  total_episodes: 1000 # Total training episodes/steps
  per_device_train_batch_size: 1 # Batch size per device
  gradient_accumulation_steps: 8 # Gradient accumulation steps
  num_wsft_epochs: 1 # Number of weighted-SFT epochs
  num_mini_batches: 1 # Number of mini batches
  kl_coef: 0.02 # KL divergence coefficient
  missing_eos_penalty: 1.0 # Missing EOS penalty
  use_lora: true # Use LoRA for efficient training
  lora_r: 16 # LoRA rank
  lora_alpha: 32 # LoRA alpha
  lora_dropout: 0.05 # LoRA dropout
  load_in_4bit: true # Load model in 4-bit quantization
  bnb_4bit_compute_dtype: bfloat16 # 4-bit compute dtype: bfloat16 or float16
  enable_thinking: false # Enable thinking mode (for Qwen3)
  save_steps: 200 # Save checkpoint every N steps
  log_steps: 50 # Log metrics every N steps
  max_train_samples: 4096 # Max training samples of one epoch
